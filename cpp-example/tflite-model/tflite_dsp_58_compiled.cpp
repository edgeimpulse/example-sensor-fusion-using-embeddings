/* Generated by Edge Impulse
 *
 * Permission is hereby granted, free of charge, to any person obtaining a copy
 * of this software and associated documentation files (the "Software"), to deal
 * in the Software without restriction, including without limitation the rights
 * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 * copies of the Software, and to permit persons to whom the Software is
 * furnished to do so, subject to the following conditions:
 *
 * The above copyright notice and this permission notice shall be included in
 * all copies or substantial portions of the Software.
 *
 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
 * SOFTWARE.
 */
// Generated on: 13.11.2023 14:11:20

#include <stdio.h>
#include <stdlib.h>
#include "edge-impulse-sdk/tensorflow/lite/c/builtin_op_data.h"
#include "edge-impulse-sdk/tensorflow/lite/c/common.h"
#include "edge-impulse-sdk/tensorflow/lite/micro/micro_mutable_op_resolver.h"
#include "edge-impulse-sdk/porting/ei_classifier_porting.h"

#if EI_CLASSIFIER_PRINT_STATE
#if defined(__cplusplus) && EI_C_LINKAGE == 1
extern "C" {
    extern void ei_printf(const char *format, ...);
}
#else
extern void ei_printf(const char *format, ...);
#endif
#endif

#if defined __GNUC__
#define ALIGN(X) __attribute__((aligned(X)))
#elif defined _MSC_VER
#define ALIGN(X) __declspec(align(X))
#elif defined __TASKING__
#define ALIGN(X) __align(X)
#elif defined __ICCARM__
#define ALIGN(x) __attribute__((aligned(x)))
#endif

#ifndef EI_MAX_SCRATCH_BUFFER_COUNT
#ifndef CONFIG_IDF_TARGET_ESP32S3
#define EI_MAX_SCRATCH_BUFFER_COUNT 4
#else
#define EI_MAX_SCRATCH_BUFFER_COUNT 8
#endif // CONFIG_IDF_TARGET_ESP32S3
#endif // EI_MAX_SCRATCH_BUFFER_COUNT

#ifndef EI_MAX_OVERFLOW_BUFFER_COUNT
#define EI_MAX_OVERFLOW_BUFFER_COUNT 10
#endif // EI_MAX_OVERFLOW_BUFFER_COUNT

using namespace tflite;
using namespace tflite::ops;
using namespace tflite::ops::micro;

namespace {

#if defined(EI_CLASSIFIER_ALLOCATION_STATIC_HIMAX) || defined(EI_CLASSIFIER_ALLOCATION_STATIC_HIMAX_GNU)
constexpr int kTensorArenaSize = 15728;
#else
constexpr int kTensorArenaSize = 14704;
#endif

#if defined(EI_CLASSIFIER_ALLOCATION_STATIC)
uint8_t tensor_arena[kTensorArenaSize] ALIGN(16);
#elif defined(EI_CLASSIFIER_ALLOCATION_STATIC_HIMAX)
#pragma Bss(".tensor_arena")
uint8_t tensor_arena[kTensorArenaSize] ALIGN(16);
#pragma Bss()
#elif defined(EI_CLASSIFIER_ALLOCATION_STATIC_HIMAX_GNU)
uint8_t tensor_arena[kTensorArenaSize] ALIGN(16) __attribute__((section(".tensor_arena")));
#else
#define EI_CLASSIFIER_ALLOCATION_HEAP 1
uint8_t* tensor_arena = NULL;
#endif

static uint8_t* tensor_boundary;
static uint8_t* current_location;

template <int SZ, class T> struct TfArray {
  int sz; T elem[SZ];
};
enum used_operators_e {
  OP_RESHAPE, OP_CONV_2D, OP_MAX_POOL_2D,  OP_LAST
};
struct TensorInfo_t { // subset of TfLiteTensor used for initialization from constant memory
  TfLiteAllocationType allocation_type;
  TfLiteType type;
  void* data;
  TfLiteIntArray* dims;
  size_t bytes;
  TfLiteQuantization quantization;
};
struct NodeInfo_t { // subset of TfLiteNode used for initialization from constant memory
  struct TfLiteIntArray* inputs;
  struct TfLiteIntArray* outputs;
  void* builtin_data;
  used_operators_e used_op_index;
};

typedef struct {
  TfLiteTensor tensor;
  int16_t index;
} TfLiteTensorWithIndex;

typedef struct {
  TfLiteEvalTensor tensor;
  int16_t index;
} TfLiteEvalTensorWithIndex;

TfLiteContext ctx{};
static const int MAX_TFL_TENSOR_COUNT = 4;
static TfLiteTensorWithIndex tflTensors[MAX_TFL_TENSOR_COUNT];
static const int MAX_TFL_EVAL_COUNT = 4;
static TfLiteEvalTensorWithIndex tflEvalTensors[MAX_TFL_EVAL_COUNT];
TfLiteRegistration registrations[OP_LAST];
TfLiteNode tflNodes[9];

const TfArray<2, int> tensor_dimension0 = { 2, { 1,6695 } };
const TfArray<1, float> quant0_scale = { 1, { 0.0035769431851804256, } };
const TfArray<1, int> quant0_zero = { 1, { -128 } };
const TfLiteAffineQuantization quant0 = { (TfLiteFloatArray*)&quant0_scale, (TfLiteIntArray*)&quant0_zero, 0 };
const ALIGN(16) int32_t tensor_data1[4] = { 1, 1, 103, 65, };
const TfArray<1, int> tensor_dimension1 = { 1, { 4 } };
const ALIGN(16) int32_t tensor_data2[4] = { 1, 103, 1, 8, };
const TfArray<1, int> tensor_dimension2 = { 1, { 4 } };
const ALIGN(16) int32_t tensor_data3[4] = { 1, 1, 52, 8, };
const TfArray<1, int> tensor_dimension3 = { 1, { 4 } };
const ALIGN(8) int32_t tensor_data4[2] = { -1, 416, };
const TfArray<1, int> tensor_dimension4 = { 1, { 2 } };
const ALIGN(16) int32_t tensor_data5[4] = { 1, 52, 1, 16, };
const TfArray<1, int> tensor_dimension5 = { 1, { 4 } };
const ALIGN(16) int32_t tensor_data6[16] = { -1809, -1767, -3471, 3592, 3684, -2694, 5068, -1662, -1564, -2798, -1589, -2908, -4035, 2905, -5380, 4733, };
const TfArray<1, int> tensor_dimension6 = { 1, { 16 } };
const TfArray<16, float> quant6_scale = { 16, { 2.1857362298760563e-05, 2.1718402422266081e-05, 3.5994402423966676e-05, 3.3676180464681238e-05, 4.015015292679891e-05, 3.1750969355925918e-05, 3.2015905162552372e-05, 3.4043456253129989e-05, 4.6060795284574851e-05, 2.5987221306422725e-05, 4.2561816371744499e-05, 2.7013464205083437e-05, 2.4969096557470039e-05, 3.2625328458379954e-05, 2.2805110347690061e-05, 3.249821747886017e-05, } };
const TfArray<16, int> quant6_zero = { 16, { 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0 } };
const TfLiteAffineQuantization quant6 = { (TfLiteFloatArray*)&quant6_scale, (TfLiteIntArray*)&quant6_zero, 0 };
const ALIGN(16) int8_t tensor_data7[16*1*3*8] = { 
  /* [0][0][][] */ 36,38,97,96,-28,-19,-127,60, 32,18,-10,-28,-72,8,-98,-95, 24,76,12,35,-87,-67,-83,29, 
  /* [1][0][][] */ -100,107,-9,127,-7,-105,-123,37, -99,-31,-64,-7,103,104,57,100, 42,9,41,-115,-52,-86,61,-14, 
  /* [2][0][][] */ -92,20,-34,15,12,-28,42,28, -5,2,45,59,44,-59,25,65, -127,26,44,-51,-15,-4,52,-10, 
  /* [3][0][][] */ 79,-40,27,-53,61,-7,-105,51, 34,-59,-28,20,-2,-49,-127,34, -42,11,-22,-49,15,-22,21,46, 
  /* [4][0][][] */ 11,-28,-37,-28,-61,-47,60,32, -25,-11,-19,-51,-28,47,127,-3, 77,51,51,-53,-53,66,71,22, 
  /* [5][0][][] */ -126,46,36,66,-14,-86,-119,72, -108,39,-15,-33,60,53,-127,-29, -103,74,51,35,6,6,-82,49, 
  /* [6][0][][] */ 114,6,18,21,-74,-22,-122,27, 84,-64,32,-60,-15,-6,-93,59, 26,-67,-59,14,2,8,-127,-49, 
  /* [7][0][][] */ -28,71,9,-37,-64,65,55,20, -48,73,50,127,56,48,1,35, -39,-7,-56,-11,79,16,13,13, 
  /* [8][0][][] */ 27,-49,42,34,14,-18,127,35, -22,-28,-41,28,49,-22,-65,61, -32,-27,36,40,14,16,-73,-3, 
  /* [9][0][][] */ -31,-61,30,80,-40,51,118,40, -70,-36,-9,-40,3,47,13,24, -51,-65,-30,127,-12,45,-46,72, 
  /* [10][0][][] */ -103,-37,-32,21,9,-38,5,-3, -127,-51,-8,-4,-25,-64,-69,23, -95,-6,-32,42,-41,16,-14,64, 
  /* [11][0][][] */ -127,52,-45,109,65,22,-55,-77, -87,-65,44,23,83,-85,-26,20, -80,-22,-65,85,-84,-92,67,-43, 
  /* [12][0][][] */ -59,51,-67,119,-30,-6,-12,-34, 4,-33,-81,-45,-86,48,-127,-24, 76,41,68,6,36,-96,-91,60, 
  /* [13][0][][] */ 44,-18,-14,-9,-77,38,95,-49, 127,30,-45,-8,-79,-80,54,-68, -10,5,83,-1,64,-56,-109,35, 
  /* [14][0][][] */ -11,24,65,-52,-26,37,-32,-65, -41,82,91,11,4,-27,-32,-63, 88,-127,-1,109,16,-117,-104,-80, 
  /* [15][0][][] */ 127,49,-16,-27,1,-17,-68,44, 60,-4,51,-32,-3,-40,105,40, 23,-58,42,-32,-86,10,97,41, 
};
const TfArray<4, int> tensor_dimension7 = { 4, { 16,1,3,8 } };
const TfArray<16, float> quant7_scale = { 16, { 0.0023995770607143641, 0.0023843215312808752, 0.00395159050822258, 0.0036970877554267645, 0.0044078230857849121, 0.0034857315476983786, 0.0035148172173649073, 0.0037374086678028107, 0.0050567137077450752, 0.0028529672417789698, 0.0046725836582481861, 0.0029656318947672844, 0.0027411940973252058, 0.0035817218013107777, 0.0025036241859197617, 0.0035677668638527393, } };
const TfArray<16, int> quant7_zero = { 16, { 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0 } };
const TfLiteAffineQuantization quant7 = { (TfLiteFloatArray*)&quant7_scale, (TfLiteIntArray*)&quant7_zero, 0 };
const ALIGN(16) int32_t tensor_data8[8] = { 26972, -6344, -4682, -22405, -7090, -6302, 27850, -5880, };
const TfArray<1, int> tensor_dimension8 = { 1, { 8 } };
const TfArray<8, float> quant8_scale = { 8, { 8.994178642751649e-06, 5.9501790019567125e-06, 5.4548768275708426e-06, 8.0425252235727385e-06, 5.4596721383859403e-06, 5.5018458624545019e-06, 1.3234282050689217e-05, 5.6089402278303169e-06, } };
const TfArray<8, int> quant8_zero = { 8, { 0,0,0,0,0,0,0,0 } };
const TfLiteAffineQuantization quant8 = { (TfLiteFloatArray*)&quant8_scale, (TfLiteIntArray*)&quant8_zero, 0 };
const ALIGN(16) int8_t tensor_data9[8*1*3*65] = { 
  /* [0][0][][] */ 60,100,82,85,12,46,92,7,-30,5,23,-67,39,-67,-59,11,-11,-34,-22,-76,10,-20,7,20,-1,-45,-33,-96,-12,-98,-87,10,-7,-39,29,-23,9,-23,41,53,8,20,-53,26,-34,-40,49,19,-31,-30,44,50,-69,38,19,25,27,-12,-50,3,-85,12,9,-12,-36, 31,113,71,115,48,16,106,-15,-42,10,25,32,12,-60,10,34,-18,42,-55,50,-81,-73,-44,-55,4,-95,-101,-17,-100,-86,-39,-67,-49,31,-44,22,-37,-1,17,17,-54,-71,3,-43,-45,-71,-68,-20,-56,-41,-37,-42,30,19,17,-70,-67,-39,2,28,-51,-88,-9,42,-61, 79,127,71,55,87,4,85,26,76,-35,70,6,-18,38,23,63,0,43,39,27,23,33,-47,-66,-77,-33,30,-66,38,15,-33,-17,-61,34,16,3,58,6,75,35,52,36,-2,-26,32,12,-35,-63,15,-39,-47,-3,-45,6,28,-33,-40,29,-2,-96,23,-63,-37,-18,51, 
  /* [1][0][][] */ -121,-108,-44,48,-127,54,-111,65,-30,-29,-51,-79,47,-96,72,27,-97,-49,97,89,42,-57,29,9,15,-62,-91,-59,-45,14,4,-109,-44,48,26,50,-68,-72,-3,-75,75,-16,-75,20,-18,92,62,87,5,-37,-15,-19,54,-102,-81,80,11,-82,-55,-40,43,93,-61,-34,-107, -43,-20,-75,-39,59,-51,-29,-33,71,-18,-42,-68,-35,-29,-118,64,-92,-6,-57,-104,54,49,-13,-106,11,-90,50,88,-80,-22,19,-36,56,77,-46,-46,11,-100,32,42,94,-57,95,-32,-78,49,-52,20,-106,-58,63,60,2,-72,46,0,77,-81,57,50,-98,39,-58,-26,34, -18,-120,67,-66,-68,-35,-85,11,38,-64,-116,28,57,-41,42,80,-92,-35,9,-10,-26,-94,2,-122,-61,-67,-94,21,-106,-90,-29,-12,-22,-28,71,51,-115,-15,61,73,56,-61,33,37,80,80,-72,78,-55,-29,42,85,66,-72,-25,5,-34,38,-73,-80,-46,67,-102,-37,-67, 
  /* [2][0][][] */ 50,84,-110,-49,-110,-111,29,38,-78,2,28,-104,32,-125,-84,46,43,-90,57,17,83,-26,-23,-10,16,-100,-102,-119,-27,-127,-98,9,-112,43,84,-82,32,-101,-37,-62,15,-21,-13,-74,-6,-18,18,-78,63,-106,87,-112,-74,1,84,39,-119,-79,49,58,-7,69,-67,-30,-84, 83,-70,-24,-108,-108,-75,57,-115,8,-50,-74,-123,31,-120,-79,35,-105,-89,-33,45,-40,87,86,43,-92,-112,-51,10,24,5,41,-105,-104,87,-120,34,20,-99,3,67,-120,-15,-85,-16,49,18,-56,-17,-112,45,-76,-13,88,-78,-90,18,42,-105,-43,-66,35,-54,-15,68,27, -38,-40,29,47,-39,37,57,49,-32,-90,-9,-90,-90,69,-46,-7,4,-50,-58,-81,79,8,-53,-8,25,-37,49,-91,-85,76,-49,7,33,8,77,2,-117,74,48,47,-52,29,-14,-78,37,-25,64,47,76,-86,-13,-44,63,30,-63,-37,-21,-121,44,31,-111,9,76,64,7, 
  /* [3][0][][] */ -105,-56,-65,-127,-1,-82,23,-17,60,-83,-1,-39,8,40,29,0,78,30,34,13,41,64,67,78,52,96,89,-29,103,61,60,67,66,35,38,29,-33,-5,-74,53,-56,-24,-1,-28,38,37,84,-30,73,50,-34,-4,-15,-22,74,-25,95,84,32,52,64,35,-3,69,0, -97,-3,-114,-30,-89,20,36,1,-10,-58,-28,12,-21,95,-15,69,7,76,-56,34,47,28,18,-48,48,120,-17,-29,50,-34,40,41,20,-20,68,-56,23,-51,14,-46,3,65,21,77,-51,77,-34,8,7,-41,-5,27,-60,-52,1,62,-39,19,68,74,-31,-47,2,-44,-34, -58,-24,-126,-42,4,-31,31,14,-40,-55,-55,62,64,76,43,-26,8,-38,-60,67,76,71,-40,-4,-6,121,39,-19,-18,4,-17,81,-36,24,32,47,48,-13,-35,-47,-38,-2,-53,91,53,49,-46,-68,25,65,-32,-31,-14,45,82,36,80,8,46,77,6,48,-57,44,19, 
  /* [4][0][][] */ 58,-84,-43,80,-100,-10,-29,78,-46,1,-95,40,46,30,-58,82,-59,-64,-92,-84,-50,-83,-84,-9,-90,-34,-47,26,59,-61,-92,21,-76,-71,50,-73,-27,91,-50,35,44,6,72,-67,-61,-12,89,-83,18,-38,-23,33,-45,47,-69,-31,-68,1,26,-64,-52,36,50,51,-53, 78,15,-81,-29,60,-90,64,-104,-85,60,27,-34,25,-114,8,73,-98,-85,3,-80,-58,41,56,-18,-64,-24,83,78,84,-93,33,-95,-76,-16,7,4,-3,-4,-49,-23,-94,-73,-32,-59,99,60,-107,97,31,-103,73,92,95,-14,-127,-22,-26,-84,24,-60,57,54,-117,-123,-31, -83,-127,-43,-87,-98,-49,-90,68,32,-122,49,-92,80,31,-22,74,-39,43,-60,-75,-112,-55,-62,-44,93,97,-82,-75,-43,-12,5,17,-75,-119,49,-59,53,34,77,-40,-106,9,-14,-55,102,-93,0,-91,-33,-22,54,49,-48,-86,75,44,29,-78,-96,38,-36,-42,49,-49,29, 
  /* [5][0][][] */ -47,11,-18,32,41,3,-72,-5,-20,61,-51,81,6,52,-46,2,-99,18,-12,77,-53,-25,86,-122,-31,-113,6,31,-90,91,-116,80,8,-40,60,3,-30,-67,72,-45,-107,-117,45,51,18,-46,-95,-76,-76,-66,-95,-91,-19,60,-51,-18,-16,-31,89,97,68,-54,42,64,-91, 11,-88,59,14,17,-69,-37,-99,29,-79,-39,16,82,-46,-94,65,-94,85,-20,7,4,-31,-53,11,86,55,27,-32,58,-79,26,67,-71,-2,-109,56,81,-75,27,-46,-89,73,60,13,-46,1,73,-43,-119,-85,-92,-20,-90,28,84,-16,88,-15,-98,34,79,22,-25,88,-75, 17,-97,-56,-118,38,28,76,-127,0,-99,19,3,69,-3,-108,46,37,-102,18,-66,11,-16,57,-75,-100,51,-36,-36,62,84,-44,52,-17,-40,41,-56,70,-124,-44,-8,84,13,-36,37,-38,88,-102,-74,57,-49,64,58,-15,-8,88,-116,38,77,-94,17,-21,-67,-64,-35,-82, 
  /* [6][0][][] */ 17,72,4,25,16,-1,2,-45,-93,-35,-51,-21,-50,-78,-11,-6,-22,47,23,3,23,-4,75,0,-18,-38,75,4,5,-9,-21,-40,5,-26,-127,-34,-95,13,-31,-36,-49,-108,-1,0,-2,77,-3,-3,5,-36,-18,50,-58,44,22,-29,-59,-31,38,10,-23,-69,-64,-61,-92, 72,50,59,30,44,96,66,-37,64,-33,54,46,32,-10,-25,20,17,21,7,-22,-12,-4,11,12,24,-53,29,-6,-23,-24,-78,9,-60,-26,-49,-66,-93,-59,-69,-21,-76,-102,-48,-49,-15,-94,-67,-66,10,-65,30,-64,-39,-88,-5,-45,-59,-4,-7,-40,-94,-76,-70,-105,-121, -21,63,41,29,20,55,-5,-31,34,-31,5,47,10,-5,-25,14,41,-23,11,68,10,51,12,0,-32,36,15,23,80,61,50,-17,13,-60,-67,-92,45,7,33,-58,-44,-32,-35,-2,30,32,7,-67,-28,8,-38,33,26,-22,-38,14,20,-49,31,0,0,-53,-22,-70,-33, 
  /* [7][0][][] */ 60,-19,34,14,59,10,-97,41,-50,-72,69,-119,43,-113,-53,7,-33,82,-60,-31,-55,20,-109,-45,49,-44,-27,59,72,61,-54,-76,46,75,-23,-92,28,-48,26,-62,-55,3,-70,-50,-109,-88,16,44,-17,8,65,-114,22,-41,-113,-75,-96,-56,-5,-98,-43,6,35,23,-16, -50,-68,42,30,-65,-120,-41,46,22,61,2,6,26,-41,-2,3,-60,-66,-67,-86,-59,-31,-10,-48,12,-41,-22,-124,2,1,47,7,47,32,-36,-42,38,43,50,-97,84,12,66,-35,42,16,58,-99,-51,-102,3,-110,-83,28,8,-81,-92,32,-28,-17,-83,26,-21,53,31, -37,53,-124,13,16,-55,20,-100,40,-74,-112,68,-127,-37,-109,49,-99,-37,-106,71,-96,-17,11,32,46,31,-9,52,-55,14,32,-8,-118,41,-34,1,-6,-111,-50,-96,-70,-11,-113,-45,-85,-85,-109,-76,-13,54,-42,64,-93,-83,-35,-17,30,17,11,18,-54,-62,86,65,-122, 
};
const TfArray<4, int> tensor_dimension9 = { 4, { 8,1,3,65 } };
const TfArray<8, float> quant9_scale = { 8, { 0.0025144873652607203, 0.0016634815838187933, 0.0015250107971951365, 0.0022484352812170982, 0.0015263513196259737, 0.0015381418634206057, 0.0036998859141021967, 0.0015680820215493441, } };
const TfArray<8, int> quant9_zero = { 8, { 0,0,0,0,0,0,0,0 } };
const TfLiteAffineQuantization quant9 = { (TfLiteFloatArray*)&quant9_scale, (TfLiteIntArray*)&quant9_zero, 0 };
const TfArray<4, int> tensor_dimension10 = { 4, { 1,1,103,65 } };
const TfArray<1, float> quant10_scale = { 1, { 0.0035769431851804256, } };
const TfArray<1, int> quant10_zero = { 1, { -128 } };
const TfLiteAffineQuantization quant10 = { (TfLiteFloatArray*)&quant10_scale, (TfLiteIntArray*)&quant10_zero, 0 };
const TfArray<4, int> tensor_dimension11 = { 4, { 1,1,103,8 } };
const TfArray<1, float> quant11_scale = { 1, { 0.0091088395565748215, } };
const TfArray<1, int> quant11_zero = { 1, { -128 } };
const TfLiteAffineQuantization quant11 = { (TfLiteFloatArray*)&quant11_scale, (TfLiteIntArray*)&quant11_zero, 0 };
const TfArray<4, int> tensor_dimension12 = { 4, { 1,103,1,8 } };
const TfArray<1, float> quant12_scale = { 1, { 0.0091088395565748215, } };
const TfArray<1, int> quant12_zero = { 1, { -128 } };
const TfLiteAffineQuantization quant12 = { (TfLiteFloatArray*)&quant12_scale, (TfLiteIntArray*)&quant12_zero, 0 };
const TfArray<4, int> tensor_dimension13 = { 4, { 1,52,1,8 } };
const TfArray<1, float> quant13_scale = { 1, { 0.0091088395565748215, } };
const TfArray<1, int> quant13_zero = { 1, { -128 } };
const TfLiteAffineQuantization quant13 = { (TfLiteFloatArray*)&quant13_scale, (TfLiteIntArray*)&quant13_zero, 0 };
const TfArray<4, int> tensor_dimension14 = { 4, { 1,1,52,8 } };
const TfArray<1, float> quant14_scale = { 1, { 0.0091088395565748215, } };
const TfArray<1, int> quant14_zero = { 1, { -128 } };
const TfLiteAffineQuantization quant14 = { (TfLiteFloatArray*)&quant14_scale, (TfLiteIntArray*)&quant14_zero, 0 };
const TfArray<4, int> tensor_dimension15 = { 4, { 1,1,52,16 } };
const TfArray<1, float> quant15_scale = { 1, { 0.0066936141811311245, } };
const TfArray<1, int> quant15_zero = { 1, { -128 } };
const TfLiteAffineQuantization quant15 = { (TfLiteFloatArray*)&quant15_scale, (TfLiteIntArray*)&quant15_zero, 0 };
const TfArray<4, int> tensor_dimension16 = { 4, { 1,52,1,16 } };
const TfArray<1, float> quant16_scale = { 1, { 0.0066936141811311245, } };
const TfArray<1, int> quant16_zero = { 1, { -128 } };
const TfLiteAffineQuantization quant16 = { (TfLiteFloatArray*)&quant16_scale, (TfLiteIntArray*)&quant16_zero, 0 };
const TfArray<4, int> tensor_dimension17 = { 4, { 1,26,1,16 } };
const TfArray<1, float> quant17_scale = { 1, { 0.0066936141811311245, } };
const TfArray<1, int> quant17_zero = { 1, { -128 } };
const TfLiteAffineQuantization quant17 = { (TfLiteFloatArray*)&quant17_scale, (TfLiteIntArray*)&quant17_zero, 0 };
const TfArray<2, int> tensor_dimension18 = { 2, { 1,416 } };
const TfArray<1, float> quant18_scale = { 1, { 0.0066936141811311245, } };
const TfArray<1, int> quant18_zero = { 1, { -128 } };
const TfLiteAffineQuantization quant18 = { (TfLiteFloatArray*)&quant18_scale, (TfLiteIntArray*)&quant18_zero, 0 };
const TfLiteReshapeParams opdata0 = { { 0, 0, 0, 0, 0, 0, 0, 0, }, 0 };
const TfArray<2, int> inputs0 = { 2, { 0,1 } };
const TfArray<1, int> outputs0 = { 1, { 10 } };
const TfLiteConvParams opdata1 = { kTfLitePaddingSame, 1,1, kTfLiteActRelu, 1,1 };
const TfArray<3, int> inputs1 = { 3, { 10,9,8 } };
const TfArray<1, int> outputs1 = { 1, { 11 } };
const TfLiteReshapeParams opdata2 = { { 0, 0, 0, 0, 0, 0, 0, 0, }, 0 };
const TfArray<2, int> inputs2 = { 2, { 11,2 } };
const TfArray<1, int> outputs2 = { 1, { 12 } };
const TfLitePoolParams opdata3 = { kTfLitePaddingSame, 1,2, 1,2, kTfLiteActNone, { { 0,0, 0, 0 } } };
const TfArray<1, int> inputs3 = { 1, { 12 } };
const TfArray<1, int> outputs3 = { 1, { 13 } };
const TfLiteReshapeParams opdata4 = { { 0, 0, 0, 0, 0, 0, 0, 0, }, 0 };
const TfArray<2, int> inputs4 = { 2, { 13,3 } };
const TfArray<1, int> outputs4 = { 1, { 14 } };
const TfLiteConvParams opdata5 = { kTfLitePaddingSame, 1,1, kTfLiteActRelu, 1,1 };
const TfArray<3, int> inputs5 = { 3, { 14,7,6 } };
const TfArray<1, int> outputs5 = { 1, { 15 } };
const TfLiteReshapeParams opdata6 = { { 0, 0, 0, 0, 0, 0, 0, 0, }, 0 };
const TfArray<2, int> inputs6 = { 2, { 15,5 } };
const TfArray<1, int> outputs6 = { 1, { 16 } };
const TfLitePoolParams opdata7 = { kTfLitePaddingSame, 1,2, 1,2, kTfLiteActNone, { { 0,0, 0, 0 } } };
const TfArray<1, int> inputs7 = { 1, { 16 } };
const TfArray<1, int> outputs7 = { 1, { 17 } };
const TfLiteReshapeParams opdata8 = { { 0, 0, 0, 0, 0, 0, 0, 0, }, 0 };
const TfArray<2, int> inputs8 = { 2, { 17,4 } };
const TfArray<1, int> outputs8 = { 1, { 18 } };
const TensorInfo_t tensorData[] = {
  { kTfLiteArenaRw, kTfLiteInt8, tensor_arena + 6704, (TfLiteIntArray*)&tensor_dimension0, 6695, {kTfLiteAffineQuantization, const_cast<void*>(static_cast<const void*>(&quant0))}, },
  { kTfLiteMmapRo, kTfLiteInt32, (void*)tensor_data1, (TfLiteIntArray*)&tensor_dimension1, 16, {kTfLiteNoQuantization, nullptr}, },
  { kTfLiteMmapRo, kTfLiteInt32, (void*)tensor_data2, (TfLiteIntArray*)&tensor_dimension2, 16, {kTfLiteNoQuantization, nullptr}, },
  { kTfLiteMmapRo, kTfLiteInt32, (void*)tensor_data3, (TfLiteIntArray*)&tensor_dimension3, 16, {kTfLiteNoQuantization, nullptr}, },
  { kTfLiteMmapRo, kTfLiteInt32, (void*)tensor_data4, (TfLiteIntArray*)&tensor_dimension4, 8, {kTfLiteNoQuantization, nullptr}, },
  { kTfLiteMmapRo, kTfLiteInt32, (void*)tensor_data5, (TfLiteIntArray*)&tensor_dimension5, 16, {kTfLiteNoQuantization, nullptr}, },
  { kTfLiteMmapRo, kTfLiteInt32, (void*)tensor_data6, (TfLiteIntArray*)&tensor_dimension6, 64, {kTfLiteAffineQuantization, const_cast<void*>(static_cast<const void*>(&quant6))}, },
  { kTfLiteMmapRo, kTfLiteInt8, (void*)tensor_data7, (TfLiteIntArray*)&tensor_dimension7, 384, {kTfLiteAffineQuantization, const_cast<void*>(static_cast<const void*>(&quant7))}, },
  { kTfLiteMmapRo, kTfLiteInt32, (void*)tensor_data8, (TfLiteIntArray*)&tensor_dimension8, 32, {kTfLiteAffineQuantization, const_cast<void*>(static_cast<const void*>(&quant8))}, },
  { kTfLiteMmapRo, kTfLiteInt8, (void*)tensor_data9, (TfLiteIntArray*)&tensor_dimension9, 1560, {kTfLiteAffineQuantization, const_cast<void*>(static_cast<const void*>(&quant9))}, },
  { kTfLiteArenaRw, kTfLiteInt8, tensor_arena + 0, (TfLiteIntArray*)&tensor_dimension10, 6695, {kTfLiteAffineQuantization, const_cast<void*>(static_cast<const void*>(&quant10))}, },
  { kTfLiteArenaRw, kTfLiteInt8, tensor_arena + 6704, (TfLiteIntArray*)&tensor_dimension11, 824, {kTfLiteAffineQuantization, const_cast<void*>(static_cast<const void*>(&quant11))}, },
  { kTfLiteArenaRw, kTfLiteInt8, tensor_arena + 0, (TfLiteIntArray*)&tensor_dimension12, 824, {kTfLiteAffineQuantization, const_cast<void*>(static_cast<const void*>(&quant12))}, },
  { kTfLiteArenaRw, kTfLiteInt8, tensor_arena + 832, (TfLiteIntArray*)&tensor_dimension13, 416, {kTfLiteAffineQuantization, const_cast<void*>(static_cast<const void*>(&quant13))}, },
  { kTfLiteArenaRw, kTfLiteInt8, tensor_arena + 0, (TfLiteIntArray*)&tensor_dimension14, 416, {kTfLiteAffineQuantization, const_cast<void*>(static_cast<const void*>(&quant14))}, },
  { kTfLiteArenaRw, kTfLiteInt8, tensor_arena + 832, (TfLiteIntArray*)&tensor_dimension15, 832, {kTfLiteAffineQuantization, const_cast<void*>(static_cast<const void*>(&quant15))}, },
  { kTfLiteArenaRw, kTfLiteInt8, tensor_arena + 0, (TfLiteIntArray*)&tensor_dimension16, 832, {kTfLiteAffineQuantization, const_cast<void*>(static_cast<const void*>(&quant16))}, },
  { kTfLiteArenaRw, kTfLiteInt8, tensor_arena + 832, (TfLiteIntArray*)&tensor_dimension17, 416, {kTfLiteAffineQuantization, const_cast<void*>(static_cast<const void*>(&quant17))}, },
  { kTfLiteArenaRw, kTfLiteInt8, tensor_arena + 0, (TfLiteIntArray*)&tensor_dimension18, 416, {kTfLiteAffineQuantization, const_cast<void*>(static_cast<const void*>(&quant18))}, },
};const NodeInfo_t nodeData[] = {
  { (TfLiteIntArray*)&inputs0, (TfLiteIntArray*)&outputs0, const_cast<void*>(static_cast<const void*>(&opdata0)), OP_RESHAPE, },
  { (TfLiteIntArray*)&inputs1, (TfLiteIntArray*)&outputs1, const_cast<void*>(static_cast<const void*>(&opdata1)), OP_CONV_2D, },
  { (TfLiteIntArray*)&inputs2, (TfLiteIntArray*)&outputs2, const_cast<void*>(static_cast<const void*>(&opdata2)), OP_RESHAPE, },
  { (TfLiteIntArray*)&inputs3, (TfLiteIntArray*)&outputs3, const_cast<void*>(static_cast<const void*>(&opdata3)), OP_MAX_POOL_2D, },
  { (TfLiteIntArray*)&inputs4, (TfLiteIntArray*)&outputs4, const_cast<void*>(static_cast<const void*>(&opdata4)), OP_RESHAPE, },
  { (TfLiteIntArray*)&inputs5, (TfLiteIntArray*)&outputs5, const_cast<void*>(static_cast<const void*>(&opdata5)), OP_CONV_2D, },
  { (TfLiteIntArray*)&inputs6, (TfLiteIntArray*)&outputs6, const_cast<void*>(static_cast<const void*>(&opdata6)), OP_RESHAPE, },
  { (TfLiteIntArray*)&inputs7, (TfLiteIntArray*)&outputs7, const_cast<void*>(static_cast<const void*>(&opdata7)), OP_MAX_POOL_2D, },
  { (TfLiteIntArray*)&inputs8, (TfLiteIntArray*)&outputs8, const_cast<void*>(static_cast<const void*>(&opdata8)), OP_RESHAPE, },
};

static void init_tflite_tensor(size_t i, TfLiteTensor *tensor) {
  tensor->type = tensorData[i].type;
  tensor->is_variable = 0;

#if defined(EI_CLASSIFIER_ALLOCATION_HEAP)
  tensor->allocation_type = tensorData[i].allocation_type;
#else
  tensor->allocation_type = (tensor_arena <= tensorData[i].data && tensorData[i].data < tensor_arena + kTensorArenaSize) ? kTfLiteArenaRw : kTfLiteMmapRo;
#endif
  tensor->bytes = tensorData[i].bytes;
  tensor->dims = tensorData[i].dims;

#if defined(EI_CLASSIFIER_ALLOCATION_HEAP)
  if(tensor->allocation_type == kTfLiteArenaRw){
    uint8_t* start = (uint8_t*) ((uintptr_t)tensorData[i].data + (uintptr_t) tensor_arena);

    tensor->data.data =  start;
  }
  else {
      tensor->data.data = tensorData[i].data;
  }
#else
  tensor->data.data = tensorData[i].data;
#endif // EI_CLASSIFIER_ALLOCATION_HEAP
  tensor->quantization = tensorData[i].quantization;
  if (tensor->quantization.type == kTfLiteAffineQuantization) {
    TfLiteAffineQuantization const* quant = ((TfLiteAffineQuantization const*)(tensorData[i].quantization.params));
    tensor->params.scale = quant->scale->data[0];
    tensor->params.zero_point = quant->zero_point->data[0];
  }

}

static void init_tflite_eval_tensor(int i, TfLiteEvalTensor *tensor) {

  tensor->type = tensorData[i].type;

  tensor->dims = tensorData[i].dims;

#if defined(EI_CLASSIFIER_ALLOCATION_HEAP)
  auto allocation_type = tensorData[i].allocation_type;
  if(allocation_type == kTfLiteArenaRw) {
    uint8_t* start = (uint8_t*) ((uintptr_t)tensorData[i].data + (uintptr_t) tensor_arena);

    tensor->data.data =  start;
  }
  else {
    tensor->data.data = tensorData[i].data;
  }
#else
  tensor->data.data = tensorData[i].data;
#endif // EI_CLASSIFIER_ALLOCATION_HEAP
}

static void* overflow_buffers[EI_MAX_OVERFLOW_BUFFER_COUNT];
static size_t overflow_buffers_ix = 0;
static void * AllocatePersistentBufferImpl(struct TfLiteContext* ctx,
                                       size_t bytes) {
  void *ptr;
  uint32_t align_bytes = (bytes % 16) ? 16 - (bytes % 16) : 0;

  if (current_location - (bytes + align_bytes) < tensor_boundary) {
    if (overflow_buffers_ix > EI_MAX_OVERFLOW_BUFFER_COUNT - 1) {
      ei_printf("ERR: Failed to allocate persistent buffer of size %d, does not fit in tensor arena and reached EI_MAX_OVERFLOW_BUFFER_COUNT\n",
        (int)bytes);
      return NULL;
    }

    // OK, this will look super weird, but.... we have CMSIS-NN buffers which
    // we cannot calculate beforehand easily.
    ptr = ei_calloc(bytes, 1);
    if (ptr == NULL) {
      ei_printf("ERR: Failed to allocate persistent buffer of size %d\n", (int)bytes);
      return NULL;
    }
    overflow_buffers[overflow_buffers_ix++] = ptr;
    return ptr;
  }

  current_location -= bytes;

  // align to the left aligned boundary of 16 bytes
  current_location -= 15; // for alignment
  current_location += 16 - ((uintptr_t)(current_location) & 15);

  ptr = current_location;
  memset(ptr, 0, bytes);

  return ptr;
}
typedef struct {
  size_t bytes;
  void *ptr;
} scratch_buffer_t;
static scratch_buffer_t scratch_buffers[EI_MAX_SCRATCH_BUFFER_COUNT];
static size_t scratch_buffers_ix = 0;

static TfLiteStatus RequestScratchBufferInArenaImpl(struct TfLiteContext* ctx, size_t bytes,
                                                int* buffer_idx) {
  if (scratch_buffers_ix > EI_MAX_SCRATCH_BUFFER_COUNT - 1) {
    ei_printf("ERR: Failed to allocate scratch buffer of size %d, reached EI_MAX_SCRATCH_BUFFER_COUNT\n",
      (int)bytes);
    return kTfLiteError;
  }

  scratch_buffer_t b;
  b.bytes = bytes;

  b.ptr = AllocatePersistentBufferImpl(ctx, b.bytes);
  if (!b.ptr) {
    ei_printf("ERR: Failed to allocate scratch buffer of size %d\n",
      (int)bytes);
    return kTfLiteError;
  }

  scratch_buffers[scratch_buffers_ix] = b;
  *buffer_idx = scratch_buffers_ix;

  scratch_buffers_ix++;

  return kTfLiteOk;
}

static void* GetScratchBufferImpl(struct TfLiteContext* ctx, int buffer_idx) {
  if (buffer_idx > (int)scratch_buffers_ix) {
    return NULL;
  }
  return scratch_buffers[buffer_idx].ptr;
}

static const uint16_t TENSOR_IX_UNUSED = 0x7FFF;

static void ResetTensors() {
  for (size_t ix = 0; ix < MAX_TFL_TENSOR_COUNT; ix++) {
    tflTensors[ix].index = TENSOR_IX_UNUSED;
  }
  for (size_t ix = 0; ix < MAX_TFL_EVAL_COUNT; ix++) {
    tflEvalTensors[ix].index = TENSOR_IX_UNUSED;
  }
}

static TfLiteTensor* GetTensorImpl(const struct TfLiteContext* context,
                               int tensor_idx) {

  for (size_t ix = 0; ix < MAX_TFL_TENSOR_COUNT; ix++) {
    // already used? OK!
    if (tflTensors[ix].index == tensor_idx) {
      return &tflTensors[ix].tensor;
    }
    // passed all the ones we've used, so end of the list?
    if (tflTensors[ix].index == TENSOR_IX_UNUSED) {
      // init the tensor
      init_tflite_tensor(tensor_idx, &tflTensors[ix].tensor);
      tflTensors[ix].index = tensor_idx;
      return &tflTensors[ix].tensor;
    }
  }

  ei_printf("ERR: GetTensor called beyond MAX_TFL_TENSOR_COUNT (%d)\n", MAX_TFL_TENSOR_COUNT);
  return nullptr;
}

static TfLiteEvalTensor* GetEvalTensorImpl(const struct TfLiteContext* context,
                                       int tensor_idx) {

  for (size_t ix = 0; ix < MAX_TFL_EVAL_COUNT; ix++) {
    // already used? OK!
    if (tflEvalTensors[ix].index == tensor_idx) {
      return &tflEvalTensors[ix].tensor;
    }
    // passed all the ones we've used, so end of the list?
    if (tflEvalTensors[ix].index == TENSOR_IX_UNUSED) {
      // init the tensor
      init_tflite_eval_tensor(tensor_idx, &tflEvalTensors[ix].tensor);
      tflEvalTensors[ix].index = tensor_idx;
      return &tflEvalTensors[ix].tensor;
    }
  }

  ei_printf("ERR: GetTensor called beyond MAX_TFL_EVAL_COUNT (%d)\n", (int)MAX_TFL_EVAL_COUNT);
  return nullptr;
}

class EonMicroContext : public MicroContext {
 public:
  EonMicroContext(): MicroContext(nullptr, nullptr, nullptr) { }

  void* AllocatePersistentBuffer(size_t bytes) {
    return AllocatePersistentBufferImpl(nullptr, bytes);
  };
  TfLiteStatus RequestScratchBufferInArena(size_t bytes,
                                           int* buffer_index) {
  return RequestScratchBufferInArenaImpl(nullptr, bytes, buffer_index);
  }
  void* GetScratchBuffer(int buffer_index) {
    return GetScratchBufferImpl(nullptr, buffer_index);
  }

  TfLiteTensor* AllocateTempTfLiteTensor(int tensor_index) {
    return GetTensorImpl(nullptr, tensor_index);
  }
  void DeallocateTempTfLiteTensor(TfLiteTensor* tensor) {
    return;
  }
  bool IsAllTempTfLiteTensorDeallocated() {
    return true;
  }

  TfLiteEvalTensor* GetEvalTensor(int tensor_index) {
    return GetEvalTensorImpl(nullptr, tensor_index);
  }
};

} // namespace

TfLiteStatus tflite_dsp_58_init( void*(*alloc_fnc)(size_t,size_t) ) {
#ifdef EI_CLASSIFIER_ALLOCATION_HEAP
  tensor_arena = (uint8_t*) alloc_fnc(16, kTensorArenaSize);
  if (!tensor_arena) {
    ei_printf("ERR: failed to allocate tensor arena\n");
    return kTfLiteError;
  }
#else
  memset(tensor_arena, 0, kTensorArenaSize);
#endif
  tensor_boundary = tensor_arena;
  current_location = tensor_arena + kTensorArenaSize;

  EonMicroContext micro_context_;
  ctx.impl_ = static_cast<void*>(&micro_context_);
  ctx.AllocatePersistentBuffer = &AllocatePersistentBufferImpl;
  ctx.RequestScratchBufferInArena = &RequestScratchBufferInArenaImpl;
  ctx.GetScratchBuffer = &GetScratchBufferImpl;
  ctx.GetTensor = &GetTensorImpl;
  ctx.GetEvalTensor = &GetEvalTensorImpl;
  ctx.tensors_size = 19;
  for (size_t i = 0; i < 19; ++i) {
    TfLiteTensor tensor;
    init_tflite_tensor(i, &tensor);
    if (tensor.allocation_type == kTfLiteArenaRw) {
      auto data_end_ptr = (uint8_t*)tensor.data.data + tensorData[i].bytes;
      if (data_end_ptr > tensor_boundary) {
        tensor_boundary = data_end_ptr;
      }
    }
  }
  if (tensor_boundary > current_location /* end of arena size */) {
    ei_printf("ERR: tensor arena is too small, does not fit model - even without scratch buffers\n");
    return kTfLiteError;
  }
  registrations[OP_RESHAPE] = Register_RESHAPE();
  registrations[OP_CONV_2D] = Register_CONV_2D();
  registrations[OP_MAX_POOL_2D] = Register_MAX_POOL_2D();

  for (size_t i = 0; i < 9; ++i) {
    tflNodes[i].inputs = nodeData[i].inputs;
    tflNodes[i].outputs = nodeData[i].outputs;
    tflNodes[i].builtin_data = nodeData[i].builtin_data;
tflNodes[i].custom_initial_data = nullptr;
      tflNodes[i].custom_initial_data_size = 0;
if (registrations[nodeData[i].used_op_index].init) {
      tflNodes[i].user_data = registrations[nodeData[i].used_op_index].init(&ctx, (const char*)tflNodes[i].builtin_data, 0);
    }
  }
  for (size_t i = 0; i < 9; ++i) {
    if (registrations[nodeData[i].used_op_index].prepare) {
      ResetTensors();

      TfLiteStatus status = registrations[nodeData[i].used_op_index].prepare(&ctx, &tflNodes[i]);
      if (status != kTfLiteOk) {
        return status;
      }
    }
  }
  return kTfLiteOk;
}

static const int inTensorIndices[] = {
  0, 
};
TfLiteStatus tflite_dsp_58_input(int index, TfLiteTensor *tensor) {
  init_tflite_tensor(inTensorIndices[index], tensor);
  return kTfLiteOk;
}

static const int outTensorIndices[] = {
  18, 
};
TfLiteStatus tflite_dsp_58_output(int index, TfLiteTensor *tensor) {
  init_tflite_tensor(outTensorIndices[index], tensor);
  return kTfLiteOk;
}

TfLiteStatus tflite_dsp_58_invoke() {
  for (size_t i = 0; i < 9; ++i) {
    ResetTensors();

    TfLiteStatus status = registrations[nodeData[i].used_op_index].invoke(&ctx, &tflNodes[i]);

#if EI_CLASSIFIER_PRINT_STATE
    ei_printf("layer %lu\n", i);
    ei_printf("    inputs:\n");
    for (size_t ix = 0; ix < tflNodes[i].inputs->size; ix++) {
      auto d = tensorData[tflNodes[i].inputs->data[ix]];

      size_t data_ptr = (size_t)d.data;

      if (d.allocation_type == kTfLiteArenaRw) {
        data_ptr = (size_t)tensor_arena + data_ptr;
      }

      if (d.type == TfLiteType::kTfLiteInt8) {
        int8_t* data = (int8_t*)data_ptr;
        ei_printf("        %lu (%zu bytes, ptr=%p, alloc_type=%d, type=%d): ", ix, d.bytes, data, (int)d.allocation_type, (int)d.type);
        for (size_t jx = 0; jx < d.bytes; jx++) {
          ei_printf("%d ", data[jx]);
        }
      }
      else {
        float* data = (float*)data_ptr;
        ei_printf("        %lu (%zu bytes, ptr=%p, alloc_type=%d, type=%d): ", ix, d.bytes, data, (int)d.allocation_type, (int)d.type);
        for (size_t jx = 0; jx < d.bytes / 4; jx++) {
          ei_printf("%f ", data[jx]);
        }
      }
      ei_printf("\n");
    }
    ei_printf("\n");

    ei_printf("    outputs:\n");
    for (size_t ix = 0; ix < tflNodes[i].outputs->size; ix++) {
      auto d = tensorData[tflNodes[i].outputs->data[ix]];

      size_t data_ptr = (size_t)d.data;

      if (d.allocation_type == kTfLiteArenaRw) {
        data_ptr = (size_t)tensor_arena + data_ptr;
      }

      if (d.type == TfLiteType::kTfLiteInt8) {
        int8_t* data = (int8_t*)data_ptr;
        ei_printf("        %lu (%zu bytes, ptr=%p, alloc_type=%d, type=%d): ", ix, d.bytes, data, (int)d.allocation_type, (int)d.type);
        for (size_t jx = 0; jx < d.bytes; jx++) {
          ei_printf("%d ", data[jx]);
        }
      }
      else {
        float* data = (float*)data_ptr;
        ei_printf("        %lu (%zu bytes, ptr=%p, alloc_type=%d, type=%d): ", ix, d.bytes, data, (int)d.allocation_type, (int)d.type);
        for (size_t jx = 0; jx < d.bytes / 4; jx++) {
          ei_printf("%f ", data[jx]);
        }
      }
      ei_printf("\n");
    }
    ei_printf("\n");
#endif // EI_CLASSIFIER_PRINT_STATE

    if (status != kTfLiteOk) {
      return status;
    }
  }
  return kTfLiteOk;
}

TfLiteStatus tflite_dsp_58_reset( void (*free_fnc)(void* ptr) ) {
#ifdef EI_CLASSIFIER_ALLOCATION_HEAP
  free_fnc(tensor_arena);
#endif

  // scratch buffers are allocated within the arena, so just reset the counter so memory can be reused
  scratch_buffers_ix = 0;

  // overflow buffers are on the heap, so free them first
  for (size_t ix = 0; ix < overflow_buffers_ix; ix++) {
    ei_free(overflow_buffers[ix]);
  }
  overflow_buffers_ix = 0;
  return kTfLiteOk;
}
